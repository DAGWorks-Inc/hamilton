{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from hamilton import base, driver\n",
    "from hamilton.io.materialization import to\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We use the autoreload extension that comes with ipython to automatically reload modules when\n",
    "# the code in them changes.\n",
    "\n",
    "# import the jupyter extension\n",
    "%load_ext autoreload\n",
    "# set it to only reload the modules imported\n",
    "%autoreload 1\n",
    "# import the function modules you want to reload when they change.\n",
    "# i.e. these should be your modules you write your functions in. As you change them,\n",
    "# they will be reimported without you having to do anything.\n",
    "%aimport my_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#   Define your new Hamilton functions & curate them into a TemporaryFunctionModule object.\n",
    "# This enables you to add functions to your DAG without creating a proper module.\n",
    "\n",
    "# Look at `my_functions` to see how these functions connect.\n",
    "def signups() -> pd.Series:\n",
    "    \"\"\"Returns sign up values\"\"\"\n",
    "    return pd.Series([1, 10, 50, 100, 200, 400])\n",
    "\n",
    "\n",
    "def spend() -> pd.Series:\n",
    "    \"\"\"Returns the spend values\"\"\"\n",
    "    return pd.Series([10, 10, 20, 40, 40, 50])\n",
    "\n",
    "\n",
    "def log_spend_per_signup(spend_per_signup: pd.Series) -> pd.Series:\n",
    "    \"\"\"Simple function taking the logarithm of spend over signups.\"\"\"\n",
    "    return np.log(spend_per_signup)\n",
    "\n",
    "\n",
    "# Place the functions into a temporary module -- the idea is that this should house a curated set of functions.\n",
    "\n",
    "temp_module = ad_hoc_utils.create_temporary_module(\n",
    "    spend, signups, log_spend_per_signup, module_name=\"function_example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "initial_columns = {  # load from actuals or wherever -- this is our initial data we use as input.\n",
    "    # Note: these values don't have to be all series, they could be a scalar.\n",
    "    \"signups\": pd.Series([1, 10, 50, 100, 200, 400]),\n",
    "    \"spend\": pd.Series([10, 10, 20, 40, 40, 50]),\n",
    "}\n",
    "# we need to tell hamilton where to load function definitions from\n",
    "# programmatic code to load modules:\n",
    "# module_name = \"my_functions\"\n",
    "# my_functions = importlib.import_module(module_name)\n",
    "# or import module(s) directly:\n",
    "\n",
    "import my_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_builder = base.PandasDataFrameResult()\n",
    "\n",
    "dr = driver.Driver({}, my_functions)  # can pass in multiple modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to specify what we want in the final dataframe. These can be string names, or function references.\n",
    "output_columns = [\n",
    "    \"spend\",\n",
    "    \"signups\",\n",
    "    \"avg_3wk_spend\",  # could just pass \"avg_3wk_spend\" here\n",
    "    \"spend_per_signup\",  # could just pass \"spend_per_signup\" here\n",
    "    \"spend_zero_mean_unit_variance\",  # could just pass \"spend_zero_mean_unit_variance\" here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materializers = [\n",
    "    # materialize the dataframe to a pickle file\n",
    "    to.pickle(\n",
    "        dependencies=output_columns,\n",
    "        id=\"df_to_pickle\",\n",
    "        path=\"./df.pkl\",\n",
    "        combine=df_builder,\n",
    "    ),\n",
    "]\n",
    "# Visualize what is happening\n",
    "dr.visualize_materialization(\n",
    "    *materializers,\n",
    "    additional_vars=output_columns,\n",
    "    output_file_path=\"./dag\",\n",
    "    render_kwargs={},\n",
    "    inputs=initial_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize a result, i.e. execute the DAG!\n",
    "materialization_results, additional_outputs = dr.materialize(\n",
    "    *materializers,\n",
    "    additional_vars=[\n",
    "        \"df_to_pickle_build_result\"\n",
    "    ],  # because combine is used, we can get that result here.\n",
    "    inputs=initial_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(materialization_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(additional_outputs[\"df_to_pickle_build_result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
